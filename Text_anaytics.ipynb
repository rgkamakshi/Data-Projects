{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "This example shows frequency based word embedding with various vectorizers.\n",
    "\n",
    "## Import packages \n",
    "\n",
    "* **punkt** - tokenizer\n",
    "* **wordnet**, **omw-1.4** - lemmatizer\n",
    "* **averaged_perceptron_tagger** - Part of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ytan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "List of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"He is a good guy, he is not bad\"\n",
    "d2 = \"feet wolves cooked boys girls ,!<@!\"\n",
    "d3 = \"He is not a good guy, he is bad\"\n",
    "\n",
    "c1 = [d1, d2, d3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "`word_tokenizer()` and `WhitespaceTokenizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10672/3124180501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_d1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_d1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWhitespaceTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtoken_d12\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "token_d1 = nltk.word_tokenize(d1)\n",
    "print(token_d1)\n",
    "\n",
    "tokenizer2 = nltk.tokenize.WhitespaceTokenizer()\n",
    "token_d12 = tokenizer2.tokenize(d1)\n",
    "print(token_d12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW)\n",
    "\n",
    "* use `CountVectorizer()`\n",
    "* `fit()` learns from docs the vocabulary indexed alphabetically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 7, 'is': 8, 'good': 5, 'guy': 6, 'not': 9, 'bad': 0, 'feet': 3, 'wolves': 10, 'cooked': 2, 'boys': 1, 'girls': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(c1)\n",
    "\n",
    "print(vectorizer1.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `transform()` create vector of word counts for each doc in the space spanned by vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 1 1 2 2 1 0]\n",
      " [0 1 1 1 1 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 1 1 2 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "v1 = vectorizer1.transform(c1)\n",
    "print(v1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "* use Porter stemmer\n",
    "* **token_d2** contains a list of token\n",
    "* for each **token**, if `isalpha()` apply the stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'wolv', 'cook', 'boy', 'girl']\n",
      "['feet', 'wolves', 'cooked', 'boys', 'girls', ',', '!', '<', '@', '!']\n"
     ]
    }
   ],
   "source": [
    "token_d2 = nltk.word_tokenize(d2.lower())\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed_token_d2 = [stemmer.stem(token) for token in token_d2 if token.isalpha()]\n",
    "\n",
    "## alternative way\n",
    "# stemmed_token_d2 = []\n",
    "# for token in token_d2:\n",
    "#     if token.isalpha():\n",
    "#         stemmed.token_d2.append(stemmer.stem(token))\n",
    "\n",
    "print(stemmed_token_d2)\n",
    "print(token_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foot', 'wolf', 'cooked', 'boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_token_d2 = [lemmatizer.lemmatize(token) for token in token_d2 if token.isalpha()]\n",
    "print(lemmatized_token_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'good', 'guy', ',', 'he', 'is', 'not', 'bad']\n",
      "['He', 'good', 'guy', 'bad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_removed = [token for token in token_d1 if not token in stopwords.words('english') \n",
    "                      if token.isalpha()]\n",
    "\n",
    "print(token_d1)\n",
    "print(stop_words_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n",
    "\n",
    "* `fit()` learns from docs the vocabulary indexed alphabetically\n",
    "* `transform()` create vector of TF-IDF measures for each doc in the space spanned by vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 7, 'is': 8, 'good': 5, 'guy': 6, 'not': 9, 'bad': 0, 'feet': 3, 'wolves': 10, 'cooked': 2, 'boys': 1, 'girls': 4}\n",
      "[[0.28867513 0.         0.         0.         0.         0.28867513\n",
      "  0.28867513 0.57735027 0.57735027 0.28867513 0.        ]\n",
      " [0.         0.4472136  0.4472136  0.4472136  0.4472136  0.\n",
      "  0.         0.         0.         0.         0.4472136 ]\n",
      " [0.28867513 0.         0.         0.         0.         0.28867513\n",
      "  0.28867513 0.57735027 0.57735027 0.28867513 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "###TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(c1)\n",
    "print(vectorizer2.vocabulary_)\n",
    "\n",
    "v2 = vectorizer2.transform(c1)\n",
    "print(v2.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of n-grams\n",
    "\n",
    "* **ngram_range** defines the range of n-grams\n",
    "* **min_df** deinfes minimum document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.45883147\n",
      "  0.45883147 0.45883147 0.22941573 0.22941573]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.45883147\n",
      "  0.45883147 0.45883147 0.22941573 0.22941573]]\n",
      "{'he': 5, 'is': 7, 'good': 1, 'guy': 3, 'not': 9, 'bad': 0, 'he is': 6, 'good guy': 2, 'guy he': 4, 'is not': 8}\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "vectorizer3.fit(c1)\n",
    "v3 = vectorizer3.transform(c1)\n",
    "\n",
    "print(v3.toarray())\n",
    "print(vectorizer3.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech (POS) tag\n",
    "\n",
    "* **`pos_tag()`** returns 2-tuple of (token, POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('drink', 'VBP'), ('water', 'NN'), ('in', 'IN'), ('parties', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "d4 = \"I drink water in parties\"\n",
    "d5 = \"I grab a drink in parties\"\n",
    "token4 = nltk.word_tokenize(d4)\n",
    "\n",
    "POS_token4 = nltk.pos_tag(token4)\n",
    "\n",
    "print(POS_token4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join each (token, POS_tag) of 2-tuple by a underscore \n",
    "* join the POS tagged tokens back to a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRP drink_VBP water_NN in_IN parties_NNS', 'I_PRP grab_VBP a_DT drink_NN in_IN parties_NNS']\n"
     ]
    }
   ],
   "source": [
    "c2 = [d4, d5]\n",
    "POS_c2 = []\n",
    "for doc in c2:\n",
    "    token_doc = nltk.word_tokenize(doc)\n",
    "    POS_token_doc = nltk.pos_tag(token_doc)\n",
    "    POS_token_temp = []\n",
    "    for i in POS_token_doc:\n",
    "        POS_token_temp.append(i[0] + \"_\" + i[1])\n",
    "    POS_c2.append(\" \".join(POS_token_temp))\n",
    "\n",
    "print(POS_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i_prp': 4, 'drink_vbp': 2, 'water_nn': 7, 'in_in': 5, 'parties_nns': 6, 'grab_vbp': 3, 'a_dt': 0, 'drink_nn': 1}\n",
      "[[0.         0.         0.53309782 0.         0.37930349 0.37930349\n",
      "  0.37930349 0.53309782]\n",
      " [0.47042643 0.47042643 0.         0.47042643 0.33471228 0.33471228\n",
      "  0.33471228 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer4 = TfidfVectorizer()\n",
    "vectorizer4.fit(POS_c2)\n",
    "print(vectorizer4.vocabulary_)\n",
    "\n",
    "POS_v3 = vectorizer4.transform(POS_c2)\n",
    "print(POS_v3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
